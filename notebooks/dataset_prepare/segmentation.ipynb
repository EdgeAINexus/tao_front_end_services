{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Segmentation dataset preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FIXME\n",
    "\n",
    "1. Assign a model_name in FIXME 1\n",
    "1. Choose between default and custom dataset in FIXME 2\n",
    "1. Assign path of DATA_DIR in FIXME 3\n",
    "1. Assign Cloud credentials in FIXME 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define model_name workspaces and other variables\n",
    "# Available models (#FIXME 1):\n",
    "# 1. segformer - https://docs.nvidia.com/tao/tao-toolkit/text/semantic_segmentation/segformer.html\n",
    "\n",
    "model_name = \"segformer\" # FIXME1 (Add the model name from the above mentioned list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example dataset source and structure <a class=\"anchor\" id=\"head-1.1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Semantic Segmentation:**\n",
    "We will be using the `ISBI Challenge: Segmentation of neuronal structures in EM stacks dataset` for the binary segmentation tutorial (Unet and Segformer). Please access the open source repo [here](https://github.com/alexklibisz/isbi-2012/tree/master/data) to download the data. The data is in .tif format. Copy the train-labels.tif, train-volume.tif, test-volume.tif files to `DATA_DIR`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**If using custom dataset; it should follow this dataset structure**\n",
    "```\n",
    "DATA_DIR\n",
    "├── images\n",
    "│   ├── test\n",
    "│   │   ├── image_0.png\n",
    "│   │   ├── image_1.png\n",
    "|   |   ├── ...\n",
    "│   ├── train\n",
    "│   │   ├── image_2.png\n",
    "│   │   ├── image_3.png\n",
    "|   |   ├── ...\n",
    "│   └── val\n",
    "│       ├── image_4.png\n",
    "│       ├── image_5.png\n",
    "|       ├── ...\n",
    "├── masks\n",
    "    ├── train\n",
    "    │   ├── image_2.png\n",
    "    │   ├── image_3.png\n",
    "    |   ├── ...\n",
    "    └── val\n",
    "        ├── image_4.png\n",
    "        ├── image_5.png\n",
    "        ├── ...\n",
    "\n",
    "```\n",
    "The filename should match for images and masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "dataset_to_be_used = \"default\" #FIXME2 #default/custom; default for the dataset used in this tutorial notebook; custom for a different dataset\n",
    "DATA_DIR = model_name #FIXME3\n",
    "os.environ['DATA_DIR']= DATA_DIR\n",
    "!mkdir -p $DATA_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset download and pre-processing <a class=\"anchor\" id=\"head-1\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Verify the downloaded dataset\n",
    "if dataset_to_be_used == \"default\":\n",
    "    assert (os.path.exists(f\"{DATA_DIR}/train-volume.tif\"))\n",
    "    assert (os.path.exists(f\"{DATA_DIR}/train-labels.tif\"))\n",
    "    assert (os.path.exists(f\"{DATA_DIR}/test-volume.tif\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if dataset_to_be_used == \"default\":\n",
    "    !python3 -m pip install Pillow opencv-python numpy\n",
    "    # create images and masks from the tif files\n",
    "    !bash unet/prepare_data.sh $DATA_DIR\n",
    "    assert (os.path.exists(f\"{DATA_DIR}/images/train\"))\n",
    "    assert (os.path.exists(f\"{DATA_DIR}/images/val\"))\n",
    "    assert (os.path.exists(f\"{DATA_DIR}/images/test\"))\n",
    "    assert (os.path.exists(f\"{DATA_DIR}/masks/train\"))\n",
    "    assert (os.path.exists(f\"{DATA_DIR}/masks/val\"))\n",
    "\n",
    "!mkdir -p $DATA_DIR/cloud_folders/data/train/images\n",
    "!mkdir -p $DATA_DIR/cloud_folders/data/train/masks\n",
    "!tar -C {DATA_DIR}/images -czf $DATA_DIR/cloud_folders/data/train/images/train.tar.gz train\n",
    "!tar -C {DATA_DIR}/images -czf $DATA_DIR/cloud_folders/data/train/images/val.tar.gz val\n",
    "!tar -C {DATA_DIR}/images -czf $DATA_DIR/cloud_folders/data/train/images/test.tar.gz test\n",
    "!tar -C {DATA_DIR}/masks -czf $DATA_DIR/cloud_folders/data/train/masks/train.tar.gz train\n",
    "!tar -C {DATA_DIR}/masks -czf $DATA_DIR/cloud_folders/data/train/masks/val.tar.gz val\n",
    "\n",
    "!mkdir -p $DATA_DIR/cloud_folders/data/val/images\n",
    "!mkdir -p $DATA_DIR/cloud_folders/data/val/masks\n",
    "!tar -C {DATA_DIR}/images -czf $DATA_DIR/cloud_folders/data/val/images/train.tar.gz train\n",
    "!tar -C {DATA_DIR}/images -czf $DATA_DIR/cloud_folders/data/val/images/val.tar.gz val\n",
    "!tar -C {DATA_DIR}/images -czf $DATA_DIR/cloud_folders/data/val/images/test.tar.gz test\n",
    "!tar -C {DATA_DIR}/masks -czf $DATA_DIR/cloud_folders/data/val/masks/train.tar.gz train\n",
    "!tar -C {DATA_DIR}/masks -czf $DATA_DIR/cloud_folders/data/val/masks/val.tar.gz val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final step: Upload the /data folder to your cloud storage and move on to running the API requests example notebooks\n",
    "When you do a ls of your bucket it should have /data folder and the subfolders we created above within in (object_detection_pyt_train, object_detection_pyt_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m pip install --upgrade awscli\n",
    "ACCESS_KEY=FIXME4.1\n",
    "SECRET_KEY=FIXME4.2\n",
    "BUCKET_NAME=FIXME4.3\n",
    "!AWS_ACCESS_KEY_ID={ACCESS_KEY} AWS_SECRET_ACCESS_KEY={SECRET_KEY} aws s3 cp {DATA_DIR}/cloud_folders/data/train s3://{BUCKET_NAME}/data/segmentation_segformer_train --recursive\n",
    "!AWS_ACCESS_KEY_ID={ACCESS_KEY} AWS_SECRET_ACCESS_KEY={SECRET_KEY} aws s3 cp {DATA_DIR}/cloud_folders/data/val s3://{BUCKET_NAME}/data/segmentation_segformer_val --recursive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# This will be the paths in your API/TAO-CLIENT Notebooks\n",
    "train_dataset_path = \"/data/segmentation_segformer_train\"\n",
    "eval_dataset_path = \"/data/segmentation_segformer_val\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
