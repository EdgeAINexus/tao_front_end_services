{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "type": "MD"
    }
   },
   "source": [
    "### TAO remote client - Optimizing YOLO\n",
    "\n",
    "Transfer learning is the process of transferring learned features from one application to another. It is a commonly used training technique where you use a model trained on one task and re-train to use it on a different task. Train Adapt Optimize (TAO) Toolkit  is a simple and easy-to-use Python based AI toolkit for taking purpose-built AI models and customizing them with users' own data.\n",
    "\n",
    "![image](https://developer.nvidia.com/sites/default/files/akamai/TAO/tlt-tao-toolkit-bring-your-own-model-diagram.png)\n",
    "\n",
    "\n",
    "### The workflow in a nutshell\n",
    "\n",
    "- Model Actions\n",
    "    - Prune, retrain\n",
    "    - Export\n",
    "    - Convert\n",
    "    - Inference on TRT\n",
    "\n",
    "### Table of contents\n",
    "\n",
    "1. [Provide TAO inference specs](#head-0)\n",
    "1. [Run TAO inference](#head-00)\n",
    "1. [Provide FP32 export specs](#head-1)\n",
    "1. [Run FP32 export](#head-2)\n",
    "1. [Provide model convert specs](#head-3)\n",
    "1. [Run model convert](#head-4)\n",
    "1. [Provide TRT inference specs](#head-5)\n",
    "1. [Run TRT inference](#head-6)\n",
    "1. [Provide prune specs](#head-7)\n",
    "1. [Run prune](#head-8)\n",
    "1. [Provide retrain specs](#head-9)\n",
    "1. [Run retrain](#head-10)\n",
    "1. [Provide evaluate specs](#head-11)\n",
    "1. [Run evaluate on retrain](#head-12)\n",
    "1. [Provide FP16 export specs](#head-13)\n",
    "1. [Run FP16 export](#head-14)\n",
    "1. [Provide model convert specs](#head-15)\n",
    "1. [Run model convert](#head-16)\n",
    "1. [Provide TRT inference specs](#head-17)\n",
    "1. [Run TRT inference](#head-18)\n",
    "1. [Delete experiment](#head-19)\n",
    "1. [Delete datasets](#head-20)\n",
    "1. [Unmount shared volume](#head-21)\n",
    "\n",
    "### Requirements\n",
    "Please find the server requirements [here](https://docs.nvidia.com/tao/tao-toolkit/text/tao_toolkit_api/api_setup.html#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "type": "CODE"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import subprocess\n",
    "import getpass\n",
    "import uuid\n",
    "import json\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restore variables set in yolo_training.ipynb\n",
    "\n",
    "with open(\"variables_to_store.json\" , \"r\") as specs_file:\n",
    "    variables_to_store = json.load(specs_file)\n",
    "\n",
    "namespace = variables_to_store[\"namespace\"]\n",
    "model_id = variables_to_store[\"model_id\"]\n",
    "train_job_id = variables_to_store[\"train_job_id\"]\n",
    "node_addr = variables_to_store[\"node_addr\"]\n",
    "node_port = variables_to_store[\"node_port\"]\n",
    "home = variables_to_store[\"home\"]\n",
    "os.environ['USER'] = variables_to_store[\"USER\"]\n",
    "os.environ['TOKEN'] = variables_to_store[\"TOKEN\"]\n",
    "\n",
    "%env BASE_URL=http://{node_addr}:{node_port}/{namespace}/api/v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def my_tail(logs_dir, log_file):\n",
    "    %env LOG_FILE={logs_dir}/{log_file}\n",
    "    ! mkdir -p {logs_dir}\n",
    "    ! [ ! -f \"$LOG_FILE\" ] && touch $LOG_FILE && chmod 666 $LOG_FILE\n",
    "    ! tail -f -n +1 $LOG_FILE | while read LINE; do echo \"$LINE\"; [[ \"$LINE\" == \"EOF\" ]] && pkill -P $$ tail; done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "logs_dir = os.path.join(home, 'shared', 'users', os.environ['USER'], 'models', model_id, 'logs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Provide TAO inference specs <a class=\"anchor\" id=\"head-0\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Default inference model specs\n",
    "! tao-client yolo-v4 model-inference-defaults --id {model_id} | tee ~/shared/users/{os.environ['USER']}/models/{model_id}/specs/inference.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Customize TAO inference specs\n",
    "specs_path = os.path.join(home, 'shared', 'users', os.environ['USER'], 'models', model_id, 'specs', 'inference.json')\n",
    "\n",
    "with open(specs_path , \"r\") as specs_file:\n",
    "    specs = json.load(specs_file)\n",
    "\n",
    "# Make any changes to specs param in the dictionary here\n",
    "specs[\"augmentation_config\"][\"output_width\"] = 960 # Setting to the Half-resolution, set this based on the dataset being used and the training/inference time tradeoff\n",
    "specs[\"augmentation_config\"][\"output_height\"] = 544 # Setting to the Half-resolution, set this based on the dataset being used and the training/inference time tradeoff\n",
    "\n",
    "with open(specs_path, \"w\") as specs_file:\n",
    "    json.dump(specs, specs_file, indent=2)\n",
    "\n",
    "print(json.dumps(specs, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run TAO inference <a class=\"anchor\" id=\"head-00\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tao_inference_job_id = subprocess.getoutput(f\"tao-client yolo-v4 model-inference --id {model_id} --job {train_job_id}\")\n",
    "print(tao_inference_job_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Check status (the file won't exist until the backend Toolkit container is running -- can take several minutes)\n",
    "log_file = f\"{tao_inference_job_id}.txt\"\n",
    "start_time = time.time()\n",
    "my_tail(logs_dir, log_file)\n",
    "tao_inference_time = time.time() - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "job_dir = f\"{home}/shared/users/{os.environ['USER']}/models/{model_id}/{tao_inference_job_id}\"\n",
    "from IPython.display import Image\n",
    "Image(filename=f\"{job_dir}/images_annotated/001354.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Provide FP32 export specs <a class=\"anchor\" id=\"head-1\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Default export model specs\n",
    "! tao-client yolo-v4 model-export-defaults --id {model_id} | tee ~/shared/users/{os.environ['USER']}/models/{model_id}/specs/export.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Customize export model specs\n",
    "specs_path = os.path.join(home, 'shared', 'users', os.environ['USER'], 'models', model_id, 'specs', 'export.json')\n",
    "\n",
    "with open(specs_path , \"r\") as specs_file:\n",
    "    specs = json.load(specs_file)\n",
    "\n",
    "# Make any changes to specs param in the dictionary here\n",
    "specs[\"data_type\"] = \"fp32\"\n",
    "specs[\"batches\"] = \"10\"\n",
    "specs[\"batch_size\"] = \"16\"\n",
    "\n",
    "with open(specs_path, \"w\") as specs_file:\n",
    "    json.dump(specs, specs_file, indent=2)\n",
    "\n",
    "print(json.dumps(specs, indent=2))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run FP32 export <a class=\"anchor\" id=\"head-2\"></a>\n",
    "* After training is completed, we need to create a onnx file which is done by export action\n",
    "* This is the intermediate step between training and creating a TRT engine file.\n",
    "* Export action modifies the original tlt model to a format which TRT engine file generation module expects\n",
    "* We can export in 3 different formats: FP32,FP16,INT8\n",
    "The export will take approximately 6 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fp32_export_job_id = subprocess.getoutput(f\"tao-client yolo-v4 model-export --id {model_id} --job {train_job_id}\")\n",
    "print(fp32_export_job_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Check status (the file won't exist until the backend Toolkit container is running -- can take several minutes)\n",
    "log_file = f\"{fp32_export_job_id}.txt\"\n",
    "my_tail(logs_dir, log_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Provide model convert specs <a class=\"anchor\" id=\"head-3\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Default convert model specs\n",
    "! tao-client yolo-v4 model-convert-defaults --id {model_id} | tee ~/shared/users/{os.environ['USER']}/models/{model_id}/specs/convert.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Customize convert model specs\n",
    "specs_path = os.path.join(home, 'shared', 'users', os.environ['USER'], 'models', model_id, 'specs', 'convert.json')\n",
    "\n",
    "with open(specs_path , \"r\") as specs_file:\n",
    "    specs = json.load(specs_file)\n",
    "\n",
    "# Make any changes to specs param in the dictionary here\n",
    "specs[\"t\"] = \"fp32\"\n",
    "specs[\"b\"] = 8\n",
    "specs[\"p\"] = \"Input,1x3x544x960,8x3x544x960,16x3x544x960\"\n",
    "\n",
    "with open(specs_path, \"w\") as specs_file:\n",
    "    json.dump(specs, specs_file, indent=2)\n",
    "\n",
    "print(json.dumps(specs, indent=2))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run model convert <a class=\"anchor\" id=\"head-4\"></a>\n",
    "Model convert action creates TRT engine file from the onnx file. This action will take approximately 6 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "convert_job_id = subprocess.getoutput(f\"tao-client yolo-v4 model-convert --id {model_id} --job {fp32_export_job_id}\")\n",
    "print(convert_job_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Check status (the file won't exist until the backend Toolkit container is running -- can take several minutes)\n",
    "log_file = f\"{convert_job_id}.txt\"\n",
    "my_tail(logs_dir, log_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Provide TRT inference specs <a class=\"anchor\" id=\"head-5\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Default inference model specs\n",
    "! tao-client yolo-v4 model-inference-defaults --id {model_id} | tee ~/shared/users/{os.environ['USER']}/models/{model_id}/specs/inference.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Customize TAO inference specs\n",
    "specs_path = os.path.join(home, 'shared', 'users', os.environ['USER'], 'models', model_id, 'specs', 'inference.json')\n",
    "\n",
    "with open(specs_path , \"r\") as specs_file:\n",
    "    specs = json.load(specs_file)\n",
    "\n",
    "# Make any changes to specs param in the dictionary here\n",
    "specs[\"augmentation_config\"][\"output_width\"] = 960 # Setting to the Half-resolution, set this based on the dataset being used and the training/inference time tradeoff\n",
    "specs[\"augmentation_config\"][\"output_height\"] = 544 # Setting to the Half-resolution, set this based on the dataset being used and the training/inference time tradeoff\n",
    "\n",
    "with open(specs_path, \"w\") as specs_file:\n",
    "    json.dump(specs, specs_file, indent=2)\n",
    "\n",
    "print(json.dumps(specs, indent=2))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run TRT inference <a class=\"anchor\" id=\"head-6\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trt_inference_job_id = subprocess.getoutput(f\"tao-client yolo-v4 model-inference --id {model_id} --job {convert_job_id}\")\n",
    "print(trt_inference_job_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Check status (the file won't exist until the backend Toolkit container is running -- can take several minutes)\n",
    "log_file = f\"{trt_inference_job_id}.txt\"\n",
    "start_time = time.time()\n",
    "my_tail(logs_dir, log_file)\n",
    "trt_inference_time = time.time() - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Time in seconds for inference on TAO model is \", tao_inference_time)\n",
    "print(\"Time in seconds for inference on FP32 TRT model is \", trt_inference_time)\n",
    "# The number shown is the total time which includes model loading + Image I/O + Inference time + post processing time\n",
    "# Full breakdown of each task given below. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Performance breakdown**\n",
    "\n",
    "**TAO model inference: <br>**\n",
    "* Load model: **~13.5s** <br>\n",
    "* Image I/O: **~9s** <br>\n",
    "* Inference Time: **~9.2s** <br>\n",
    "* Post Process Time: **~5.9s** <br>\n",
    "\n",
    "**TensorRT model inference: <br>**\n",
    "* Load model: **~1.3s** <br>\n",
    "* Image I/O: **~9s** <br>\n",
    "* Inference Time: **~3.4s** <br>\n",
    "* Post Process Time: **~5.9s** <br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "job_dir = f\"{home}/shared/users/{os.environ['USER']}/models/{model_id}/{trt_inference_job_id}\"\n",
    "from IPython.display import Image\n",
    "Image(filename=f\"{job_dir}/images_annotated/001354.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "type": "MD"
    }
   },
   "source": [
    "### Provide prune specs <a class=\"anchor\" id=\"head-7\"></a>\n",
    "To control the pruned model size, the user can change the following parameters of prune action:\n",
    "1. pruning_threshold - The threshold to compare a normalized norm against (default: 0.1)\n",
    "1. pruning_granularity - The number of filters to remove at a time (default: 8)\n",
    "1. min_num_filters - The minimum number of filters to keep per layer (default: 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "type": "CODE"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Default prune model specs\n",
    "! tao-client yolo-v4 model-prune-defaults --id {model_id} | tee ~/shared/users/{os.environ['USER']}/models/{model_id}/specs/prune.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Customize prune model specs\n",
    "specs_path = os.path.join(home, 'shared', 'users', os.environ['USER'], 'models', model_id, 'specs', 'prune.json')\n",
    "\n",
    "with open(specs_path , \"r\") as specs_file:\n",
    "    specs = json.load(specs_file)\n",
    "\n",
    "# Make any changes to specs param in the dictionary here\n",
    "specs[\"pruning_threshold\"] = 0.8\n",
    "\n",
    "with open(specs_path, \"w\") as specs_file:\n",
    "    json.dump(specs, specs_file, indent=2)\n",
    "\n",
    "print(json.dumps(specs, indent=2))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "type": "MD"
    }
   },
   "source": [
    "### Run prune <a class=\"anchor\" id=\"head-8\"></a>\n",
    "This job will take approximately 7 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "type": "CODE"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "prune_job_id = subprocess.getoutput(f\"tao-client yolo-v4 model-prune --id {model_id} --job {train_job_id}\")\n",
    "print(prune_job_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Check status (the file won't exist until the backend Toolkit container is running -- can take several minutes)\n",
    "log_file = f\"{prune_job_id}.txt\"\n",
    "my_tail(logs_dir, log_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prune_job_dir = f\"{home}/shared/users/{os.environ['USER']}/models/{model_id}/{prune_job_id}\"\n",
    "pruned_model_size = json.loads(subprocess.getoutput(f'stat -c \"%s\" {prune_job_dir}/model.tlt'))\n",
    "\n",
    "train_job_dir = f\"{home}/shared/users/{os.environ['USER']}/models/{model_id}/{train_job_id}\"\n",
    "original_train_model_size = json.loads(subprocess.getoutput(f'stat -c \"%s\" {train_job_dir}/weights/yolov4_resnet18_epoch_010.tlt'))\n",
    "\n",
    "print(f\"The original trained model size is {original_train_model_size} KB\")\n",
    "print(f\"The pruned model size is {pruned_model_size} KB\")\n",
    "print(f\"The pruned model is {round(original_train_model_size/pruned_model_size,1)}x smaller than the original model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "type": "MD"
    }
   },
   "source": [
    "### Provide retrain specs <a class=\"anchor\" id=\"head-9\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "type": "CODE"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Default retrain model specs\n",
    "! tao-client yolo-v4 model-retrain-defaults --id {model_id} | tee ~/shared/users/{os.environ['USER']}/models/{model_id}/specs/retrain.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Customize retrain model specs\n",
    "specs_path = os.path.join(home, 'shared', 'users', os.environ['USER'], 'models', model_id, 'specs', 'retrain.json')\n",
    "\n",
    "with open(specs_path , \"r\") as specs_file:\n",
    "    specs = json.load(specs_file)\n",
    "\n",
    "# Make any changes to specs param in the dictionary here\n",
    "specs[\"training_config\"][\"num_epochs\"] = 150\n",
    "specs[\"augmentation_config\"][\"output_width\"] = 960 # Setting to the Half-resolution, set this based on the dataset being used and the training/inference time tradeoff\n",
    "specs[\"augmentation_config\"][\"output_height\"] = 544 # Setting to the Half-resolution, set this based on the dataset being used and the training/inference time tradeoff\n",
    "specs[\"dataset_config\"][\"image_extension\"] = \"jpg\" # Setting to the dataset's image_file extension type\n",
    "\n",
    "with open(specs_path, \"w\") as specs_file:\n",
    "    json.dump(specs, specs_file, indent=2)\n",
    "\n",
    "print(json.dumps(specs, indent=2))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "type": "MD"
    }
   },
   "source": [
    "### Run retrain <a class=\"anchor\" id=\"head-10\"></a>\n",
    "Model needs to be re-trained to bring back accuracy after pruning. Re-training will take approximately 1 hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "type": "CODE"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "retrain_job_id = subprocess.getoutput(f\"tao-client yolo-v4 model-retrain --id {model_id} --job {prune_job_id}\")\n",
    "print(retrain_job_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Check status (the file won't exist until the backend Toolkit container is running -- can take several minutes)\n",
    "log_file = f\"{retrain_job_id}.txt\"\n",
    "my_tail(logs_dir, log_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Provide evaluate specs <a class=\"anchor\" id=\"head-11\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Default evaluate model specs\n",
    "! tao-client yolo-v4 model-evaluate-defaults --id {model_id} | tee ~/shared/users/{os.environ['USER']}/models/{model_id}/specs/evaluate.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Customize evaluate model specs\n",
    "specs_path = os.path.join(home, 'shared', 'users', os.environ['USER'], 'models', model_id, 'specs', 'evaluate.json')\n",
    "\n",
    "with open(specs_path , \"r\") as specs_file:\n",
    "    specs = json.load(specs_file)\n",
    "\n",
    "# Make any changes to specs param in the dictionary here\n",
    "specs[\"augmentation_config\"][\"output_width\"] = 960 # Setting to the Half-resolution, set this based on the dataset being used and the training/inference time tradeoff\n",
    "specs[\"augmentation_config\"][\"output_height\"] = 544 # Setting to the Half-resolution, set this based on the dataset being used and the training/inference time tradeoff\n",
    "specs[\"dataset_config\"][\"image_extension\"] = \"jpg\" # Setting to the dataset's image_file extension type\n",
    "\n",
    "with open(specs_path, \"w\") as specs_file:\n",
    "    json.dump(specs, specs_file, indent=2)\n",
    "\n",
    "print(json.dumps(specs, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run evaluate on retrained model <a class=\"anchor\" id=\"head-12\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "eval2_job_id = subprocess.getoutput(f\"tao-client yolo-v4 model-evaluate --id {model_id} --job {retrain_job_id}\")\n",
    "print(eval2_job_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Check status (the file won't exist until the backend Toolkit container is running -- can take several minutes)\n",
    "log_file = f\"{eval2_job_id}.txt\"\n",
    "my_tail(logs_dir, log_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Provide FP16 export specs <a class=\"anchor\" id=\"head-13\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Default export model specs\n",
    "! tao-client yolo-v4 model-export-defaults --id {model_id} | tee ~/shared/users/{os.environ['USER']}/models/{model_id}/specs/export.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Customize export model specs\n",
    "specs_path = os.path.join(home, 'shared', 'users', os.environ['USER'], 'models', model_id, 'specs', 'export.json')\n",
    "\n",
    "with open(specs_path , \"r\") as specs_file:\n",
    "    specs = json.load(specs_file)\n",
    "\n",
    "# Make any changes to specs param in the dictionary here\n",
    "specs[\"data_type\"] = \"fp16\"\n",
    "specs[\"batches\"] = 10\n",
    "specs[\"batch_size\"] = 16\n",
    "\n",
    "with open(specs_path, \"w\") as specs_file:\n",
    "    json.dump(specs, specs_file, indent=2)\n",
    "\n",
    "print(json.dumps(specs, indent=2))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run FP16 export <a class=\"anchor\" id=\"head-14\"></a>\n",
    "The export will take about 12 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "export_job_id_2 = subprocess.getoutput(f\"tao-client yolo-v4 model-export --id {model_id} --job {retrain_job_id}\")\n",
    "print(export_job_id_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Check status (the file won't exist until the backend Toolkit container is running -- can take several minutes)\n",
    "log_file = f\"{export_job_id_2}.txt\"\n",
    "my_tail(logs_dir, log_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Provide model convert specs <a class=\"anchor\" id=\"head-15\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Default convert model specs\n",
    "! tao-client yolo-v4 model-convert-defaults --id {model_id} | tee ~/shared/users/{os.environ['USER']}/models/{model_id}/specs/convert.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Customize convert model specs\n",
    "specs_path = os.path.join(home, 'shared', 'users', os.environ['USER'], 'models', model_id, 'specs', 'convert.json')\n",
    "\n",
    "with open(specs_path , \"r\") as specs_file:\n",
    "    specs = json.load(specs_file)\n",
    "\n",
    "# Make any changes to specs param in the dictionary here\n",
    "specs[\"t\"] = \"fp16\"\n",
    "specs[\"b\"] = 8\n",
    "specs[\"p\"] = \"Input,1x3x544x960,8x3x544x960,16x3x544x960\"\n",
    "\n",
    "with open(specs_path, \"w\") as specs_file:\n",
    "    json.dump(specs, specs_file, indent=2)\n",
    "\n",
    "print(json.dumps(specs, indent=2))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run model convert <a class=\"anchor\" id=\"head-16\"></a>\n",
    "The convert operation will take approximately 10 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "convert_job_id_2 = subprocess.getoutput(f\"tao-client yolo-v4 model-convert --id {model_id} --job {export_job_id_2}\")\n",
    "print(convert_job_id_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Check status (the file won't exist until the backend Toolkit container is running -- can take several minutes)\n",
    "log_file = f\"{convert_job_id_2}.txt\"\n",
    "my_tail(logs_dir, log_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Provide TRT inference specs <a class=\"anchor\" id=\"head-17\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Default inference model specs\n",
    "! tao-client yolo-v4 model-inference-defaults --id {model_id} | tee ~/shared/users/{os.environ['USER']}/models/{model_id}/specs/inference.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Customize TAO inference specs\n",
    "specs_path = os.path.join(home, 'shared', 'users', os.environ['USER'], 'models', model_id, 'specs', 'inference.json')\n",
    "\n",
    "with open(specs_path , \"r\") as specs_file:\n",
    "    specs = json.load(specs_file)\n",
    "\n",
    "# Make any changes to specs param in the dictionary here\n",
    "specs[\"augmentation_config\"][\"output_width\"] = 960 # Setting to the Half-resolution, set this based on the dataset being used and the training/inference time tradeoff\n",
    "specs[\"augmentation_config\"][\"output_height\"] = 544 # Setting to the Half-resolution, set this based on the dataset being used and the training/inference time tradeoff\n",
    "\n",
    "with open(specs_path, \"w\") as specs_file:\n",
    "    json.dump(specs, specs_file, indent=2)\n",
    "\n",
    "print(json.dumps(specs, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run TRT inference <a class=\"anchor\" id=\"head-18\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tao_inference_job_id_2 = subprocess.getoutput(f\"tao-client yolo-v4 model-inference --id {model_id} --job {convert_job_id_2}\")\n",
    "print(tao_inference_job_id_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Check status (the file won't exist until the backend Toolkit container is running -- can take several minutes)\n",
    "log_file = f\"{tao_inference_job_id_2}.txt\"\n",
    "start_time = time.time()\n",
    "my_tail(logs_dir, log_file)\n",
    "inference_time = time.time() - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Time in seconds for inference on unoptimized FP32 TRT model is \", trt_inference_time)\n",
    "print(\"Time in seconds for inference on pruned FP16  model is \", inference_time)\n",
    "# The number shown is the total time which includes model loading + Image I/O + Inference time + post processing time\n",
    "# Full breakdown of each task given below. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Performance breakdown\n",
    "**TensorRT unoptimized FP32 model inference: <br>**\n",
    "* Load model: **~1.3s** <br>\n",
    "* Image I/O: **~9s** <br>\n",
    "* Inference Time: **~3.4s** <br>\n",
    "* Post Process Time: **~5.9s** <br>\n",
    "\n",
    "**TensorRT pruned, optimized FP16 model inference: <br>**\n",
    "* Load model: **~1.3s** <br>\n",
    "* Image I/O: **~9s** <br>\n",
    "* Inference Time: **~2.2s** <br>\n",
    "* Post Process Time: **~5.9s** <br>\n",
    "\n",
    "Inference Time Speedup from unoptimized FP32 model to pruned FP16 model: **~55%**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "job_dir = f\"{home}/shared/users/{os.environ['USER']}/models/{model_id}/{tao_inference_job_id_2}\"\n",
    "from IPython.display import Image\n",
    "Image(filename=f\"{job_dir}/images_annotated/001354.jpg\")"
   ]
  }
 ],
 "metadata": {
  "datalore": {
   "base_environment": "default",
   "computation_mode": "JUPYTER",
   "package_manager": "pip",
   "packages": [],
   "version": 1
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
